{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<img src=\"./image/Logo/logo_elia_group.png\" width = 200>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Manipulation\n",
    "<br> \n",
    "\n",
    "In order to work with your data set, you oftentimes need to prepare your data. For instance, you might need to get rid of missing values, rename columns or recode values so that your data makes sense. This step is called **data cleaning**.\n",
    "<br>\n",
    "But don't worry, with Pandas, you can do more than just select data that is already there. You can add new columns to your datasets, apply functions, iterate through each row in the dataframe, and more. This is where you move from \"pandas for exploring our data\" to **\"pandas for getting your data ready to fit into models\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/Icons/cleaning.png\" width = 200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import one package that you are going to need in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as two data sets, one of which you already know: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power = pd.read_csv('data/energy/ods032.csv', parse_dates=True, sep = \";\", index_col=0)\n",
    "energy = pd.read_csv('data/energy/elia_load_2019_01_15.csv', parse_dates=True, sep = \";\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start to clean them up, always take a look at them. This is super important in order to understand your data and how it is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Values\n",
    "\n",
    "The first thing you want to do when cleaning your data is to check whether it consists of missing values. Missing values are blank cells as well as NaN or n/a values. In Pandas, these entries will be treated as null vallues. In Python, you can...\n",
    "\n",
    "1. identify missing values\n",
    "2. drop missing values\n",
    "3. or replace or fill missing values\n",
    "\n",
    "With the syntax: `df_name.isnull().any().any()` you can check your data frame and see whether there are missing values. <br> But first, let's go through the command step by step and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Does the pv_power df contain nulls?:', pv_power.isnull().any().any())\n",
    "print('Does the energy df contain nulls?:', energy.isnull().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** why did we use \".any()\" twice when asking this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `pv_power` contains some missing values. Let's check which columns contain them and how many:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to deal with the null values in the `pv_power` table. How you deal with them depends on the data type of the column in which null values are found and of course how you intend to use the data. For instance, if your `pv_power` data doesn't have a `Resolution code` listed, you can still get a lot of information about it from things like its `Region`. But if you want to look at the photovoltaic power of Belgium and there is no `Monitored capacity`, it might be diffcult to interpret your results. <br>\n",
    "\n",
    "When you replace, delete or fill missing values, it might affect your analysis at a later stage because you change the dataset. This is why it is important for others and for yourself that you document well the underlying assumptions.\n",
    "You have different options on how to deal with this missing values. You could:\n",
    "- delete every missing row `.dropna(subset = [\"column\"])`\n",
    "- copy the value from the time stamp before `.fillna(method = \"ffill\", inplace = True)`\n",
    "- copy the value from the time stamp after `.fillna(method = \"bfill\", inplace = True)`\n",
    "- insert the overall mean `.fillna(df[\"column\"].mean(), inplace = True)`\n",
    "- and more.  <br>  \n",
    "\n",
    "(\"column\" needs to be replaced by the column name in which you like to modify the missing values.)  \n",
    "After analysing the data, you decide to **drop the rows with null values** in `Measured & Upscaled` and **replace the null values** in `Day Ahead 11am forecast` and `Week-ahead forecast` with the value from the time stamp before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Null Values\n",
    "\n",
    "Let's drop the rows with null values in the column `Measured & Upscaled`. To do so...\n",
    "\n",
    "1. Look at the column your choice and check its null values, as well as how many rows are effected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power[pv_power[\"Measured & Upscaled\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power[pv_power[\"Measured & Upscaled\"].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at the data frame, you can see, that the \"newer\" data misses values (data from the 21th of June 2022). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Continue and check how many rows the original data frame has: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power[\"Measured & Upscaled\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, drop all the 1302 rows with NaN values and save it into the data frame before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power = pv_power.dropna(subset=['Measured & Upscaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#128077; Well done, now 1302 rows where a NaN value existed for `Measured & Upscaled` have been dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Null Values\n",
    "\n",
    "To not delete all rows and end up with basically no data left, you can replace null values - **if it makes sense**. <br>\n",
    "Let's check where null values are left: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's replace all null values with the last non missing value before the missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power[\"Day Ahead 11AM forecast\"].fillna(method = \"ffill\", inplace=True)\n",
    "pv_power[\"Week-ahead forecast\"].fillna(method = \"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "&#128515; Amazing, now you know how to deal with missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns from existing ones\n",
    "<br>\n",
    "\n",
    "The last thing you have to know about data cleaning and manipulation, is how to create new columns from existing ones. This can come in handy in many cases. For instance, you might need to calculate the mean load factor and save these values in a new column. Can you think of other use cases? <br>\n",
    "\n",
    "In pandas, it's easy to make a new column from existing ones. Just look at the example below and check the syntax yourself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.loc[:, 'New_column'] = pv_power.loc[:, 'Resolution code'] + '_' + pv_power.loc[:, 'Region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe you want to convert MW into kW for the column `Monitored capacity`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.loc[:, 'Monit_cap_kW'] = pv_power.loc[:, 'Monitored capacity'] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_power.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Recap, Tips & Takeaways &#128161;\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Let's recap what you have learned so far:**\n",
    "\n",
    "- Data Cleaning is one of the most important steps when working with your data \n",
    "- You can drop missing values with: `df_name.dropna(subset=['column_name'])`\n",
    "- you have different options to replace missing values with: `df_name.fillna()`\n",
    "- To check for missing values you can use `df_name.isnull().any().any()`\n",
    "- **iloc** works **positionally** with numbers (in our case the index)\n",
    "- **loc** searches for **labels**\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra resources\n",
    "To know more functions available in a dataframe read:\n",
    "- [Dataframe documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
